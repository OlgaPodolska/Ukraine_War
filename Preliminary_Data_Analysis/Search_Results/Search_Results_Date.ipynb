{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d509fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import aylien_news_api\n",
    "from aylien_news_api.rest import ApiException\n",
    "from pprint import pprint\n",
    "configuration = aylien_news_api.Configuration()\n",
    "# Configure API key authorization: app_id\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-ID'] = '8b0eec79'\n",
    "# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n",
    "# configuration.api_key_prefix['X-AYLIEN-NewsAPI-Application-ID'] = 'Bearer'\n",
    "configuration = aylien_news_api.Configuration()\n",
    "# Configure API key authorization: app_key\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-Key'] = '34a3f65df4fc4033015bb15fa654202b'\n",
    "# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n",
    "# configuration.api_key_prefix['X-AYLIEN-NewsAPI-Application-Key'] = 'Bearer'\n",
    "\n",
    "# Defining host is optional and default to https://api.aylien.com/news\n",
    "configuration.host = \"https://api.aylien.com/news\"\n",
    "# Create an instance of the API class\n",
    "api_instance = aylien_news_api.DefaultApi(aylien_news_api.ApiClient(configuration))\n",
    "\n",
    "opts = {\n",
    "  'id': [56],\n",
    "  'not_id': [56],\n",
    "  'title': 'title_example',\n",
    "  'body': 'body_example',\n",
    "  'text': 'text_example',\n",
    "  'translations_en_title': 'translations_en_title_example',\n",
    "  'translations_en_body': 'translations_en_body_example',\n",
    "  'translations_en_text': 'translations_en_text_example',\n",
    "  'language': ['language_example'],\n",
    "  'not_language': ['language_example'],\n",
    "  'published_at_start': 'published_at_start_example',\n",
    "  'published_at_end': 'published_at_end_example',\n",
    "  'categories_taxonomy': 'categories_taxonomy_example',\n",
    "  'categories_confident': True,\n",
    "  'categories_id': ['categories_id_example'],\n",
    "  'not_categories_id': ['categories_id_example'],\n",
    "  'categories_level': [56],\n",
    "  'not_categories_level': [56],\n",
    "  'entities_id': ['entities_id_example'],\n",
    "  'not_entities_id': ['entities_id_example'],\n",
    "  'entities_surface_forms_text': ['entities_surface_form_text_example'],\n",
    "  'not_entities_surface_forms_text': ['entities_surface_form_text_example'],\n",
    "  'entities_types': ['entities_types_example'],\n",
    "  'not_entities_types': ['entities_types_example']\n",
    "  'entities_links_wikipedia': ['entities_links_wikipedia_example'],\n",
    "  'not_entities_links_wikipedia': ['entities_links_wikipedia_example'],\n",
    "  'entities_links_wikidata': ['entities_links_wikidata_example'],\n",
    "  'not_entities_links_wikidata': ['entities_links_wikidata_example'],\n",
    "  'entities_stock_ticker': ['entities_stock_ticker_example'],\n",
    "  'not_entities_stock_ticker': ['entities_stock_ticker_example'],\n",
    "  'entities_title_surface_forms_text': ['entities_title_surface_form_text_example'],\n",
    "  'not_entities_title_surface_forms_text': ['entities_title_surface_form_text_example'],\n",
    "  'entities_body_surface_forms_text': ['entities_body_surface_form_text_example'],\n",
    "  'not_entities_body_surface_forms_text': ['entities_body_surface_form_text_example'],\n",
    "  'sentiment_title_polarity': 'sentiment_title_polarity_example',\n",
    "  'not_sentiment_title_polarity': 'sentiment_title_polarity_example',\n",
    "  'sentiment_body_polarity': 'sentiment_body_polarity_example',\n",
    "  'not_sentiment_body_polarity': 'sentiment_body_polarity_example',\n",
    "  'media_images_count_min': 56,\n",
    "  'media_images_count_max': 56,\n",
    "  'media_images_width_min': 56,\n",
    "  'media_images_width_max': 56,\n",
    "  'media_images_height_min': 56,\n",
    "  'media_images_height_max': 56,\n",
    "  'media_images_content_length_min': 56,\n",
    "  'media_images_content_length_max': 56,\n",
    "  'media_images_format': ['media_images_format_example'],\n",
    "  'not_media_images_format': ['media_images_format_example'],\n",
    "  'media_videos_count_min': 56,\n",
    "  'media_videos_count_max': 56,\n",
    "  'author_id': [56],\n",
    "  'not_author_id': [56],\n",
    "  'author_name': 'author_name_example',\n",
    "  'not_author_name': 'author_name_example',\n",
    "  'source_id': [56],\n",
    "  'not_source_id': [56],\n",
    "  'source_name': ['source_name_example'],\n",
    "  'not_source_name': ['source_name_example'],\n",
    "  'source_domain': ['source_domain_example'],\n",
    "  'not_source_domain': ['source_domain_example'],\n",
    "  'source_locations_country': ['source_locations_country_example'],\n",
    "  'not_source_locations_country': ['source_locations_country_example'],\n",
    "  'source_locations_state': ['source_locations_state_example'],\n",
    "  'not_source_locations_state': ['source_locations_state_example'],\n",
    "  'source_locations_city': ['source_locations_city_example'],\n",
    "  'not_source_locations_city': ['source_locations_city_example'],\n",
    "  'source_scopes_country': ['source_scopes_country_example'],\n",
    "  'not_source_scopes_country': ['source_scopes_country_example'],\n",
    "  'source_scopes_state': ['source_scopes_state_example'],\n",
    "  'not_source_scopes_state': ['source_scopes_state_example'],\n",
    "  'source_scopes_city': ['source_scopes_city_example'],\n",
    "  'not_source_scopes_city': ['source_scopes_city_example'],\n",
    "  'source_scopes_level': ['source_scopes_level_example'],\n",
    "  'not_source_scopes_level': ['source_scopes_level_example'],\n",
    "  'source_links_in_count_min': 56,\n",
    "  'source_links_in_count_max': 56,\n",
    "  'source_rankings_alexa_rank_min': 56,\n",
    "  'source_rankings_alexa_rank_max': 56,\n",
    "  'source_rankings_alexa_country': ['source_rankings_alexa_country_example'],\n",
    "  'social_shares_count_facebook_min': 56,\n",
    "  'social_shares_count_facebook_max': 56,\n",
    "  'social_shares_count_google_plus_min': 56,\n",
    "  'social_shares_count_google_plus_max': 56,\n",
    "  'social_shares_count_linkedin_min': 56,\n",
    "  'social_shares_count_linkedin_max': 56,\n",
    "  'social_shares_count_reddit_min': 56,\n",
    "  'social_shares_count_reddit_max': 56,\n",
    "  'clusters': ['clusters_example'],\n",
    "  '_return': ['_return_example'],\n",
    "  'story_id': 56,\n",
    "  'story_url': 'story_url_example',\n",
    "  'story_title': 'story_title_example',\n",
    "  'story_body': 'story_body_example',\n",
    "  'story_published_at': '2013-10-20T19:20:30+01:00',\n",
    "  'story_language': 'auto',\n",
    "  'aql': 'aql_example'\n",
    "  'per_page': 3\n",
    "}\n",
    "try:\n",
    "    # List coverages\n",
    "    api_response = api_instance.list_coverages(**opts)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DefaultApi->list_coverages: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6d67784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-24\n",
      "2022-02-25\n",
      "2022-02-26\n",
      "2022-02-27\n",
      "2022-02-28\n",
      "2022-03-01\n",
      "2022-03-02\n",
      "2022-03-03\n",
      "2022-03-04\n",
      "2022-03-05\n",
      "2022-03-06\n",
      "2022-03-07\n",
      "2022-03-08\n",
      "2022-03-09\n",
      "2022-03-10\n",
      "2022-03-11\n",
      "2022-03-12\n",
      "2022-03-13\n",
      "2022-03-14\n",
      "2022-03-15\n",
      "2022-03-16\n",
      "2022-03-17\n",
      "2022-03-18\n",
      "2022-03-19\n",
      "2022-03-20\n",
      "2022-03-21\n",
      "2022-03-22\n",
      "2022-03-23\n",
      "2022-03-24\n",
      "2022-03-25\n",
      "2022-03-26\n",
      "2022-03-27\n",
      "2022-03-28\n",
      "2022-03-29\n",
      "2022-03-30\n",
      "2022-03-31\n",
      "2022-04-01\n",
      "2022-04-02\n",
      "2022-04-03\n",
      "2022-04-04\n",
      "2022-04-05\n",
      "2022-04-06\n",
      "2022-04-07\n",
      "2022-04-08\n",
      "2022-04-09\n",
      "2022-04-10\n",
      "2022-04-11\n",
      "2022-04-12\n",
      "2022-04-13\n",
      "2022-04-14\n",
      "2022-04-15\n",
      "2022-04-16\n",
      "2022-04-17\n",
      "2022-04-18\n",
      "2022-04-19\n",
      "2022-04-20\n",
      "2022-04-21\n",
      "2022-04-22\n",
      "2022-04-23\n",
      "2022-04-24\n",
      "2022-04-25\n",
      "2022-04-26\n",
      "2022-04-27\n",
      "2022-04-28\n",
      "2022-04-29\n",
      "2022-04-30\n",
      "2022-05-01\n",
      "2022-05-02\n",
      "2022-05-03\n",
      "2022-05-04\n",
      "2022-05-05\n",
      "2022-05-06\n",
      "2022-05-07\n",
      "2022-05-08\n",
      "2022-05-09\n",
      "2022-05-10\n",
      "2022-05-11\n",
      "2022-05-12\n",
      "2022-05-13\n",
      "2022-05-14\n",
      "2022-05-15\n",
      "2022-05-16\n",
      "2022-05-17\n",
      "2022-05-18\n",
      "2022-05-19\n",
      "2022-05-20\n",
      "2022-05-21\n",
      "2022-05-22\n",
      "2022-05-23\n",
      "2022-05-24\n",
      "2022-05-25\n",
      "2022-05-26\n",
      "2022-05-27\n",
      "2022-05-28\n",
      "2022-05-29\n",
      "2022-05-30\n",
      "2022-05-31\n",
      "2022-06-01\n",
      "2022-06-02\n",
      "2022-06-03\n",
      "2022-06-04\n",
      "2022-06-05\n",
      "2022-06-06\n",
      "2022-06-07\n",
      "2022-06-08\n",
      "2022-06-09\n",
      "2022-06-10\n",
      "2022-06-11\n",
      "2022-06-12\n",
      "2022-06-13\n",
      "2022-06-14\n",
      "2022-06-15\n",
      "2022-06-16\n",
      "2022-06-17\n",
      "2022-06-18\n",
      "2022-06-19\n",
      "2022-06-20\n",
      "2022-06-21\n",
      "2022-06-22\n",
      "2022-06-23\n",
      "2022-06-24\n",
      "2022-06-25\n",
      "2022-06-26\n",
      "2022-06-27\n",
      "2022-06-28\n",
      "2022-06-29\n",
      "2022-06-30\n",
      "2022-07-01\n",
      "2022-07-02\n",
      "2022-07-03\n",
      "2022-07-04\n",
      "2022-07-05\n",
      "2022-07-06\n",
      "2022-07-07\n",
      "2022-07-08\n",
      "2022-07-09\n",
      "2022-07-10\n",
      "2022-07-11\n",
      "2022-07-12\n",
      "2022-07-13\n",
      "2022-07-14\n",
      "2022-07-15\n",
      "2022-07-16\n",
      "2022-07-17\n",
      "2022-07-18\n",
      "2022-07-19\n",
      "2022-07-20\n",
      "2022-07-21\n",
      "2022-07-22\n",
      "2022-07-23\n",
      "2022-07-24\n",
      "2022-07-25\n",
      "2022-07-26\n",
      "2022-07-27\n",
      "2022-07-28\n",
      "2022-07-29\n",
      "2022-07-30\n",
      "2022-07-31\n",
      "2022-08-01\n",
      "2022-08-02\n",
      "2022-08-03\n",
      "2022-08-04\n",
      "2022-08-05\n",
      "2022-08-06\n",
      "2022-08-07\n",
      "2022-08-08\n",
      "2022-08-09\n",
      "2022-08-10\n",
      "2022-08-11\n"
     ]
    }
   ],
   "source": [
    "# Loop through February 24th through August 12th \n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = date(2022, 2, 24)\n",
    "end_date = date(2022, 8, 12)\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    print(single_date.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5280b849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date\n",
      "0    2022-03-06\n",
      "1    2022-05-14\n",
      "2    2022-06-09\n",
      "3    2022-04-17\n",
      "4    2022-05-26\n",
      "..          ...\n",
      "164  2022-03-19\n",
      "165  2022-07-30\n",
      "166  2022-03-08\n",
      "167  2022-05-24\n",
      "168  2022-07-02\n",
      "\n",
      "[169 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe for the dates \n",
    "# importing pandas\n",
    "import pandas as pd\n",
    " \n",
    "# Creating new dataframe\n",
    "initial_data = {'2022-02-24','2022-02-25','2022-02-26','2022-02-27','2022-02-28','2022-03-01','2022-03-02',\n",
    "'2022-03-03','2022-03-04','2022-03-05','2022-03-06','2022-03-07','2022-03-08','2022-03-09','2022-03-10',\n",
    "'2022-03-11','2022-03-12','2022-03-13','2022-03-14','2022-03-15','2022-03-16','2022-03-17','2022-03-18','2022-03-19','2022-03-20','2022-03-21',\n",
    "'2022-03-22','2022-03-23','2022-03-24','2022-03-25','2022-03-26','2022-03-27','2022-03-28','2022-03-29','2022-03-30',\n",
    "'2022-03-31','2022-04-01','2022-04-02','2022-04-03','2022-04-04','2022-04-05','2022-04-06','2022-04-07','2022-04-08',\n",
    "'2022-04-09','2022-04-10','2022-04-11','2022-04-12','2022-04-13','2022-04-14','2022-04-15','2022-04-16','2022-04-17',\n",
    "'2022-04-18','2022-04-19','2022-04-20','2022-04-21','2022-04-22','2022-04-23','2022-04-24','2022-04-25','2022-04-26',\n",
    "'2022-04-27','2022-04-28','2022-04-29','2022-04-30','2022-05-01','2022-05-02','2022-05-03','2022-05-04','2022-05-05',\n",
    "'2022-05-06','2022-05-07','2022-05-08','2022-05-09','2022-05-10','2022-05-11','2022-05-12','2022-05-13','2022-05-14',\n",
    "'2022-05-15','2022-05-16','2022-05-17','2022-05-18','2022-05-19','2022-05-20','2022-05-21','2022-05-22','2022-05-23',\n",
    "'2022-05-24','2022-05-25','2022-05-26','2022-05-27','2022-05-28','2022-05-29','2022-05-30','2022-05-31','2022-06-01','2022-06-02',\n",
    "'2022-06-03','2022-06-04','2022-06-05','2022-06-06','2022-06-07','2022-06-08','2022-06-09','2022-06-10','2022-06-11',\n",
    "'2022-06-12','2022-06-13','2022-06-14','2022-06-15','2022-06-16','2022-06-17','2022-06-18','2022-06-19','2022-06-20',\n",
    "'2022-06-21','2022-06-22','2022-06-23','2022-06-24','2022-06-25','2022-06-26','2022-06-27','2022-06-28','2022-06-29',\n",
    "'2022-06-30','2022-07-01','2022-07-02','2022-07-03','2022-07-04','2022-07-05','2022-07-06','2022-07-07','2022-07-08',\n",
    "'2022-07-09','2022-07-10','2022-07-11','2022-07-12','2022-07-13','2022-07-14','2022-07-15','2022-07-16','2022-07-17','2022-07-18','2022-07-19','2022-07-20','2022-07-21',\n",
    "'2022-07-22','2022-07-23','2022-07-24','2022-07-25','2022-07-26','2022-07-27','2022-07-28','2022-07-29','2022-07-30',\n",
    "'2022-07-31','2022-08-01','2022-08-02','2022-08-03','2022-08-04','2022-08-05','2022-08-06','2022-08-07','2022-08-08','2022-08-09','2022-08-10','2022-08-11'}\n",
    " \n",
    "date_df = pd.DataFrame(initial_data, columns = ['Date'])\n",
    "\n",
    "print(date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b81cfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date      \n",
       "2022-02-24    1\n",
       "2022-06-20    1\n",
       "2022-06-12    1\n",
       "2022-06-13    1\n",
       "2022-06-14    1\n",
       "             ..\n",
       "2022-04-23    1\n",
       "2022-04-24    1\n",
       "2022-04-25    1\n",
       "2022-04-26    1\n",
       "2022-08-11    1\n",
       "Length: 169, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b831ebe",
   "metadata": {},
   "source": [
    "## Manually getting number of search results per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b9ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b070470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n",
      "\n",
      "<html>\n",
      "<head><meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/><meta content=\"initial-scale=1\" name=\"viewport\"/><title>https://www.google.com/search?q=ukraine+war+articles</title></head>\n",
      "<body onload=\"e=document.getElementById('captcha');if(e){e.focus();} if(solveSimpleChallenge) {solveSimpleChallenge(,);}\" style=\"font-family: arial, sans-serif; background-color: #fff; color: #000; padding:20px; font-size:18px;\">\n",
      "<div style=\"max-width:400px;\">\n",
      "<hr noshade=\"\" size=\"1\" style=\"color:#ccc; background-color:#ccc;\"/><br/>\n",
      "<form action=\"index\" id=\"captcha-form\" method=\"post\">\n",
      "<noscript>\n",
      "<div style=\"font-size:13px;\">\n",
      "  In order to continue, please enable javascript on your web browser.\n",
      "</div>\n",
      "</noscript>\n",
      "<script async=\"\" defer=\"\" src=\"https://www.google.com/recaptcha/api.js\"></script>\n",
      "<script>var submitCallback = function(response) {document.getElementById('captcha-form').submit();};</script>\n",
      "<div class=\"g-recaptcha\" data-callback=\"submitCallback\" data-s=\"09uJL0kkPD9MQv__k2l5eNArisWL822tCyLZ-ohFO1Um1YL6r-3-R6rU9c0kW1S0Q2pP5N3zIeH0AX7JqaQVtBixeJqHaEo3i5N-HRJaPvChFfXAaUbYn_SlWX5SL3VueDjA5eOfernvkHI6bH6vexv1-x24p9q6oTLmGsL5lq72SaF7ifIKnvp--N5VCW_B_GDEE2YvVY_PJOEt2Lf7I_CIGx6FdPD0qLz2vto\" data-sitekey=\"6LfwuyUTAAAAAOAmoS0fdqijC2PbbdH4kjq62Y1b\" id=\"recaptcha\"></div>\n",
      "<input name=\"q\" type=\"hidden\" value=\"EgSIGW4-GIr-tZgGIhBlUua6gkTKz-h810KbwCeGMgFy\"/><input name=\"continue\" type=\"hidden\" value=\"https://www.google.com/search?q=ukraine+war+articles\"/>\n",
      "</form>\n",
      "<hr noshade=\"\" size=\"1\" style=\"color:#ccc; background-color:#ccc;\"/>\n",
      "<div style=\"font-size:13px;\">\n",
      "<b>About this page</b><br/><br/>\n",
      "\n",
      "Our systems have detected unusual traffic from your computer network.  This page checks to see if it's really you sending the requests, and not a robot.  <a href=\"#\" onclick=\"document.getElementById('infoDiv').style.display='block';\">Why did this happen?</a><br/><br/>\n",
      "<div id=\"infoDiv\" style=\"display:none; background-color:#eee; padding:10px; margin:0 0 15px 0; line-height:1.4em;\">\n",
      "This page appears when Google automatically detects requests coming from your computer network which appear to be in violation of the <a href=\"//www.google.com/policies/terms/\">Terms of Service</a>. The block will expire shortly after those requests stop.  In the meantime, solving the above CAPTCHA will let you continue to use our services.<br/><br/>This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests.  If you share your network connection, ask your administrator for help â€” a different computer using the same IP address may be responsible.  <a href=\"//support.google.com/websearch/answer/86640\">Learn more</a><br/><br/>Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending requests very quickly.\n",
      "</div>\n",
      "\n",
      "IP address: 136.25.110.62<br/>Time: 2022-08-30T03:07:54Z<br/>URL: https://www.google.com/search?q=ukraine+war+articles<br/>\n",
      "</div>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of search results \n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles\"\n",
    "\n",
    "result = requests.get(URL, headers=headers) \n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "# total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "# results_num_total = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "# print(results_num_total)\n",
    "result = requests.get(URL, headers=headers) \n",
    "soup = BeautifulSoup(result.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of search results \n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles\"\n",
    "\n",
    "result = requests.get(URL, headers=headers) \n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num_total = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb03bb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_41448/197217712.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtotal_results_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"result-stats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will give you the outer text which is like 'About 1,410,000,000 results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mresults_num1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_results_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now will clean it up and remove all the characters that are not a number .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for 2/24/22\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+2%2F24%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num1 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b098a4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+2%2F25%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num2 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3370a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+2%2F26%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num3 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6f93f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+2%2F27%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num4 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2578ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+2%2F28%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num5 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9fd566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F1%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num6 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8686a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F1%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num7 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bcc0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F2%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num8 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de802348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F3%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num9 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fdec665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "# https://stackoverflow.com/questions/61064420/how-to-print-the-number-of-google-search-results-beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F4%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num10 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "347b9e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F5%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num10 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c975939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F6%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num11 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25053ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F7%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num12 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4d8a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F8%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num13 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c688de75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F9%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num14 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "156015bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F10%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num15 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38179a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F11%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num16 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27704ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F12%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num17 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0f79c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90500\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F13%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num18 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bfde028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F14%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num19 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b92a8daf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_37554/4084141952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtotal_results_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"result-stats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will give you the outer text which is like 'About 1,410,000,000 results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresults_num20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_results_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now will clean it up and remove all the characters that are not a number .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F15%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num20 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6085f94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_37554/2975633142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtotal_results_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"result-stats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will give you the outer text which is like 'About 1,410,000,000 results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresults_num21\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_results_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now will clean it up and remove all the characters that are not a number .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F16%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num21 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b259fcb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_37554/2741924194.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtotal_results_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"result-stats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will give you the outer text which is like 'About 1,410,000,000 results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresults_num22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_results_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now will clean it up and remove all the characters that are not a number .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F17%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num22 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e970706",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_37554/2019575377.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtotal_results_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"result-stats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will give you the outer text which is like 'About 1,410,000,000 results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresults_num23\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_results_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now will clean it up and remove all the characters that are not a number .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F18%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num23 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fedb891f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_37554/815425164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtotal_results_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"result-stats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will give you the outer text which is like 'About 1,410,000,000 results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresults_num24\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_results_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now will clean it up and remove all the characters that are not a number .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F19%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num24 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffefd38f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_37554/2503118972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtotal_results_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"result-stats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will give you the outer text which is like 'About 1,410,000,000 results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresults_num25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_results_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now will clean it up and remove all the characters that are not a number .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F20%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num25 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f44af2b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_37554/3246660826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtotal_results_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"result-stats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will give you the outer text which is like 'About 1,410,000,000 results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresults_num19\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_results_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now will clean it up and remove all the characters that are not a number .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Get the number of search results for next day\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
    "URL     = \"https://www.google.com/search?q=ukraine+war+articles+3%2F14%2F22\"\n",
    "result = requests.get(URL, headers=headers)    \n",
    "\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
    "results_num19 = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
    "print(results_num19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e6ffb50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_41556/3436873885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         }\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mSearchDate_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSearchDate_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return arrays_to_mgr(\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Create a data with what I have already\n",
    "import pandas as pd\n",
    "\n",
    "data = {'date':  ['2022-02-24','2022-02-25','2022-02-26','2022-02-27','2022-02-28','2022-03-01','2022-03-02',\n",
    "'2022-03-03','2022-03-04','2022-03-05','2022-03-06','2022-03-07','2022-03-08','2022-03-09','2022-03-10',\n",
    "'2022-03-11','2022-03-12','2022-03-13','2022-03-14','2022-03-15','2022-03-16','2022-03-17','2022-03-18','2022-03-19','2022-03-20','2022-03-21',\n",
    "'2022-03-22','2022-03-23','2022-03-24','2022-03-25','2022-03-26','2022-03-27','2022-03-28','2022-03-29','2022-03-30',\n",
    "'2022-03-31','2022-04-01','2022-04-02','2022-04-03','2022-04-04','2022-04-05','2022-04-06','2022-04-07','2022-04-08',\n",
    "'2022-04-09','2022-04-10','2022-04-11','2022-04-12','2022-04-13','2022-04-14','2022-04-15','2022-04-16','2022-04-17',\n",
    "'2022-04-18','2022-04-19','2022-04-20','2022-04-21','2022-04-22','2022-04-23','2022-04-24','2022-04-25','2022-04-26',\n",
    "'2022-04-27','2022-04-28','2022-04-29','2022-04-30','2022-05-01','2022-05-02','2022-05-03','2022-05-04','2022-05-05',\n",
    "'2022-05-06','2022-05-07','2022-05-08','2022-05-09','2022-05-10','2022-05-11','2022-05-12','2022-05-13','2022-05-14',\n",
    "'2022-05-15','2022-05-16','2022-05-17','2022-05-18','2022-05-19','2022-05-20','2022-05-21','2022-05-22','2022-05-23',\n",
    "'2022-05-24','2022-05-25','2022-05-26','2022-05-27','2022-05-28','2022-05-29','2022-05-30','2022-05-31','2022-06-01','2022-06-02',\n",
    "'2022-06-03','2022-06-04','2022-06-05','2022-06-06','2022-06-07','2022-06-08','2022-06-09','2022-06-10','2022-06-11',\n",
    "'2022-06-12','2022-06-13','2022-06-14','2022-06-15','2022-06-16','2022-06-17','2022-06-18','2022-06-19','2022-06-20',\n",
    "'2022-06-21','2022-06-22','2022-06-23','2022-06-24','2022-06-25','2022-06-26','2022-06-27','2022-06-28','2022-06-29',\n",
    "'2022-06-30','2022-07-01','2022-07-02','2022-07-03','2022-07-04','2022-07-05','2022-07-06','2022-07-07','2022-07-08',\n",
    "'2022-07-09','2022-07-10','2022-07-11','2022-07-12','2022-07-13','2022-07-14','2022-07-15','2022-07-16','2022-07-17','2022-07-18','2022-07-19','2022-07-20','2022-07-21',\n",
    "'2022-07-22','2022-07-23','2022-07-24','2022-07-25','2022-07-26','2022-07-27','2022-07-28','2022-07-29','2022-07-30',\n",
    "'2022-07-31','2022-08-01','2022-08-02','2022-08-03','2022-08-04','2022-08-05','2022-08-06','2022-08-07','2022-08-08','2022-08-09','2022-08-10','2022-08-11'],\n",
    "        'search_results': ['0', '206000', '106000', '111000', '142000', '306000', '0', '236000', '251000', '0', '158000', '212000',\n",
    "                          '138000', '120000', '191000','160000', '165000', '90500', '104000','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0''0','0','0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0','0','0','0']\n",
    "        }\n",
    "\n",
    "SearchDate_df = pd.DataFrame(data)\n",
    "\n",
    "print(SearchDate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "395c75ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "date = {'2022-02-24','2022-02-25','2022-02-26','2022-02-27','2022-02-28','2022-03-01','2022-03-02',\n",
    "'2022-03-03','2022-03-04','2022-03-05','2022-03-06','2022-03-07','2022-03-08','2022-03-09','2022-03-10',\n",
    "'2022-03-11','2022-03-12','2022-03-13','2022-03-14','2022-03-15','2022-03-16','2022-03-17','2022-03-18','2022-03-19','2022-03-20','2022-03-21',\n",
    "'2022-03-22','2022-03-23','2022-03-24','2022-03-25','2022-03-26','2022-03-27','2022-03-28','2022-03-29','2022-03-30',\n",
    "'2022-03-31','2022-04-01','2022-04-02','2022-04-03','2022-04-04','2022-04-05','2022-04-06','2022-04-07','2022-04-08',\n",
    "'2022-04-09','2022-04-10','2022-04-11','2022-04-12','2022-04-13','2022-04-14','2022-04-15','2022-04-16','2022-04-17',\n",
    "'2022-04-18','2022-04-19','2022-04-20','2022-04-21','2022-04-22','2022-04-23','2022-04-24','2022-04-25','2022-04-26',\n",
    "'2022-04-27','2022-04-28','2022-04-29','2022-04-30','2022-05-01','2022-05-02','2022-05-03','2022-05-04','2022-05-05',\n",
    "'2022-05-06','2022-05-07','2022-05-08','2022-05-09','2022-05-10','2022-05-11','2022-05-12','2022-05-13','2022-05-14',\n",
    "'2022-05-15','2022-05-16','2022-05-17','2022-05-18','2022-05-19','2022-05-20','2022-05-21','2022-05-22','2022-05-23',\n",
    "'2022-05-24','2022-05-25','2022-05-26','2022-05-27','2022-05-28','2022-05-29','2022-05-30','2022-05-31','2022-06-01','2022-06-02',\n",
    "'2022-06-03','2022-06-04','2022-06-05','2022-06-06','2022-06-07','2022-06-08','2022-06-09','2022-06-10','2022-06-11',\n",
    "'2022-06-12','2022-06-13','2022-06-14','2022-06-15','2022-06-16','2022-06-17','2022-06-18','2022-06-19','2022-06-20',\n",
    "'2022-06-21','2022-06-22','2022-06-23','2022-06-24','2022-06-25','2022-06-26','2022-06-27','2022-06-28','2022-06-29',\n",
    "'2022-06-30','2022-07-01','2022-07-02','2022-07-03','2022-07-04','2022-07-05','2022-07-06','2022-07-07','2022-07-08',\n",
    "'2022-07-09','2022-07-10','2022-07-11','2022-07-12','2022-07-13','2022-07-14','2022-07-15','2022-07-16','2022-07-17','2022-07-18','2022-07-19','2022-07-20','2022-07-21',\n",
    "'2022-07-22','2022-07-23','2022-07-24','2022-07-25','2022-07-26','2022-07-27','2022-07-28','2022-07-29','2022-07-30',\n",
    "'2022-07-31','2022-08-01','2022-08-02','2022-08-03','2022-08-04','2022-08-05','2022-08-06','2022-08-07','2022-08-08','2022-08-09','2022-08-10','2022-08-11'}\n",
    "\n",
    "\n",
    "print(len(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64fd8bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "search_results = {'0', '206000', '106000', '111000', '142000', '306000', '0', '236000', '251000', '0', '158000', '212000',\n",
    "                          '138000', '120000', '191000','160000', '165000', '90500', '104000','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0''0','0','0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0',\n",
    "'0','0','0','0','0','0','0','0','0','0','0','0'\n",
    "        }\n",
    "    \n",
    "print(len(search_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40470d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
